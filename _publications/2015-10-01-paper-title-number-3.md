---
title: "Hardening LLM Fine-Tuning: From Differentially Private Data Selection to Trustworthy Model Quantization"
collection: publications
category: manuscripts
permalink: /publication/2015-10-01-paper-title-number-3
excerpt: 'Critical infrastructures are increasingly integrating artificial intelligence (AI) technologies, including large language models (LLMs), into essential systems and services that are vital to societal functioning. Fine-tuning LLMs for specific domain tasks are crucial for their effective deployment in these contexts, but this process must carefully address both privacy and security concerns. Without proper safeguards, such integration can introduce additional risks, such as data leakage during training and diminished model trustworthiness due to the need for model compression to operate within limited bandwidth and computational capacity constraints. In this paper, we propose Hardening LLM Fine-tuning framework (HardLLM), which addresses these challenges through two key components: (i) we develop a differentially private data selection method that ensures privacy protection by training the model exclusively on sampled and synthesized public data, thereby preventing any direct use of private data and enhancing leakage resilience throughout the training process, and (ii) we introduce a trustworthiness-aware model quantization approach to improve LLMs performance, such as reducing toxicity, enhancing adversarial robustness, and mitigating stereotypes, while maintaining negligible impact on model utility.'
date: 2025-06-18
venue: 'IEEE Transactions on Information Forensics and Security '
slidesurl: 
paperurl: 'https://ieeexplore.ieee.org/abstract/document/11040052'
citation: 'Deng, Zehang, Ruoxi Sun, Minhui Xue, Wanlun Ma, Sheng Wen, Surya Nepal, and Yang Xiang. "Hardening LLM Fine-Tuning: From Differentially Private Data Selection to Trustworthy Model Quantization." IEEE Transactions on Information Forensics and Security (2025).'
---
